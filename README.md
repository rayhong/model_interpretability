Presented by: [Ray Hong](http://www.rayhong.net), [Jessica Hullman](http://users.eecs.northwestern.edu/~jhullman/), and [Enrico Bertini](http://enrico.bertini.io/)

## Background

With the rapid increase in the deployment of machine learning (ML) in a wide range of practical application areas, we are witnissing increased concern about our capabilities to understand _how_ ML models work; shifting our focus on interacting with ML from a unidirectional focus on _model accuracy_ to a larger perspective that includes a strong focus on ML _model interpretability_. Despite this rapid expansion in model interpretability tools and formal and empirical definitions, our understanding of how interpretability is understood by and how it impacts the actual ML practitioners is still limited. Bridging this gap with emperical approach is necessary for grounding our work in the real problems we face when dealing with interpretability issues.

## Methodology

This project presents our results of empirical inquiry; we conducted semi-structured interviews with 22 ML practitioners from 20 different companies across 5 months of a period, who work in wide variety of domains including banking, finance, healthcare, IT, social media, consulting, manufacturing, internet services, and transportation. They are responsible for building models that are state-of-the-art in their field; some of the models they built are used by millions of daily active users and/or support critical decisions where life is at stake (17 data scientists or machine learning engineers (DS), 2 software engineers who build infrastructure related to model interpretability (SE), 2 product managers (PM), and oneUX researcher (UX), see the table below).

| PID | Company Domain     | Job title and role                | Domain problems                              |
| :-- | :----------------- | :-------------------------------- | -------------------------------------------- |
| P1  | Software           | Staff manager in Research (DS)    | Object detection for telepresence robots     |
| P2  | Consulting         | CEO (DS)                          | Identity recognition, fraud detection        |
| P3  | Banking            | Senior ML Engineer (DS)           | Credit risk assessment, call transcription   |
| P4  | Software           | Lead Data Scientist (DS)          | Sentiment analysis, object detection, AutoML |
| P5  | Banking            | Data Engineer (SE)                | Anomalous recurring online payment detection |
| P6  | Banking/Finance    | Head of AI (DS)                   | Credit evaluation for business loans         |
| P7  | Internet Services  | Senior Software Developer (SE)    | Tooling for tabular/image data               |
| P8  | Social Media       | Data Scientist (DS)               | Service user retention prediction            |
| P9  | Banking/Finance    | Principal Data Scientist          | Customer acquisition/financial forcasting    |
| P10 | Software           | Product Manager (PM)              | Tooling (visualizing model interpretation)   |
| P11 | Consulting         | Lead Data Scientist (DS)          | Revenue maximization strategy prediction     |
| P12 | Internet Services  | Head of ML (DS)                   | Oversees more than 50 models                 |
| P13 | Transportation     | Senior Software Engineer (DS)     | Place recommendation, partners acquisition   |
| P14 | Social Media       | ML Engineering Manager (DS)       | Advertisement ranking                        |
| P15 | Transportation S/W | Senior ML Research Scientist (DS) | Claim handling, driver voice assistance      |
| P16 | Healthcare         | Medical Doctor (DS)               | Mortality, deterioration, and radiology      |
| P17 | Manufacturing      | Senior Researh Scientist (DS)     | Oil pump/power grid operation                |
| P18 | Software/Research  | CTO (SE)                          | Tooling (image-based model interpretation)   |
| P19 | Finance            | Senior Analytics Expert (PM)      | Credit score prediction                      |
| P20 | Healthcare         | CTO, Head of Data Science (DS)    | Lung cancer operation                        |
| P21 | Software           | Principal Design Manager (UX)     | Best images selection, transcription         |
| P22 | Healthcare         | Senior Data Scientist (DS)        | Care management/resource utilization         |

## Results

In total, we collected 19 hours and 10 minutes of audio recorded interviews, and conducted qualitative approach. The analyses we present represent the perspectives of our participants systematically curated in an interpretive framework which was developed through discussions among the authors. Our interviewing approach aimed to differentiate how our interviewees perceived interpretability _roles_, _processes_, _goals_, and associated _challenges and design opportunities_. We will briefly cover some notable challenges and design opportunity; if interested, our full manuscript is availabe as follows:

[https://arxiv.org/abs/2004.11440](https://arxiv.org/abs/2004.11440)

### 1

blah

### 2

blah

## Cite this work

Bibtex:

```
@article{hong2019design,
  title={Human Factors in Model Interpretability:Industry Practices},
  author={Hong, Sungsoo Ray and Hullman, Jessica and Bertini, Enrico},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={4},
  number={CSCW1},
  pages={1--26},
  year={2020},
  publisher={ACM New York, NY, USA},
  DOI={10.1145/3392878}
}
```

APA in-text:

```
Sungsoo Ray Hong, Jessica Hullman, and Enrico Bertini. 2020.
Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs.
Proc. ACM Hum.-Comput. Interact.4, CSCW1, Article 311 (May 2020), 26 pages. https://doi.org/10.1145/33928781
```
